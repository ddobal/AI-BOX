---
layout: post
title: "강화학습 환경 설계 포인트"
date: 2025-12-03
category: "Reinforcement Learning"
tags: [rl, reward-design, observation, trading]
---

강화학습(RL)을 처음 접하면 보통 알고리즘(DQN, PPO, SAC 등)에 시선이 쏠리지만,  
직접 해보면 결국 중요한 것은 **환경 설계**라는 것을 느끼게 된다.  
이 글은 트레이딩 환경을 예로, 환경 설계 시 고민했던 포인트를 정리한 기록이다.

---

## 1. 관측 공간(Observation Space)

트레이딩 환경에서 에이전트가 볼 수 있는 정보는 다음과 비슷하게 구성했다.

- 최근 N개의 캔들(OHLCV)
- 보유 포지션 정보 (롱/숏/노포지션, 수량, 평균 단가)
- 간단한 기술 지표 (RSI, 이동평균선 등)

느낀 점:

- **정보가 많다고 좋은 게 아니다.**  
  - 처음에는 지표를 마구 넣었더니 학습이 불안정해졌다.  
  - 최소한의 정보로 시작하고, 하나씩 추가하는 방식이 더 낫다.

---

## 2. 보상 함수(Reward) 설계

가장 많이 시행착오를 겪은 부분이다.

- 단순히 “포지션 종료 시 PnL만 보상”을 줬더니:
  - 에이전트가 아무 행동도 하지 않거나,
  - 극단적인 레버리지/진입을 시도하는 경우가 많았다.
- 개선 방향:
  - 과한 포지션에 패널티
  - 장기간 포지션 홀딩에 대한 비용(시간 비용)
  - 극단적인 변동성 구간에서의 리스크 반영

결론적으로,  
**“지나치게 단순한 보상 = 이상한 행동”** 이라는 것을 몸으로 배웠다.

---

## 3. 에피소드 설계

트레이딩 환경에서는 에피소드 길이도 중요하다.

- 너무 짧으면:  
  - 장기적인 전략을 학습하기 어렵다.
- 너무 길면:  
  - Credit Assignment 문제가 심해지고, 학습이 불안정해진다.

그래서 일정 길이의 구간을 에피소드로 자르고,  
여러 시점에서 무작위로 시작하는 방식을 사용했다.

---

## 4. 안전장치와 규제

실제 환경에 가까운 RL을 생각하면,  
다음과 같은 제한도 고려할 필요가 있다.

- 최대 손실 제한 (max drawdown)
- 1회 진입 가능 수량 제한
- 레버리지 상한선

이런 제한들을 환경에 포함하면,  
에이전트가 “현실적인 행동”을 더 잘 학습한다.

---

## 5. 정리 및 회고

- RL에서 “알고리즘 선택”보다 **환경 설계**가 훨씬 힘들다.
- 관측, 보상, 제약 조건이 서로 맞물려 돌아갈 때  
  비로소 의미 있는 정책이 나온다.
- 아직은 실험 단계지만,  
  앞으로 더 다양한 보상 구조와 에피소드 설계를 테스트해볼 계획이다.
